{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhVtckeopYtr5b92AvORCa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamonSaturninoM/NLP_TextSegmentation/blob/master/nlpSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSC 446/646: Natural Language Processing, Assignment 1"
      ],
      "metadata": {
        "id": "Nn44og7AUys4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import spacy\n",
        "import nltk\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "HQT76wgMN0tP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDq4g963cC8V",
        "outputId": "daa4f7d0-9c46-4b9a-abf2-7f5ad4e77f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "trlJOPkDf2ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Rule-based NLP"
      ],
      "metadata": {
        "id": "Cw_BX1SqUn4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip dataset\n",
        "def extract_dataset(path, extract_to=\"dataset\"):\n",
        "  with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "  files = os.listdir(extract_to)\n",
        "  return [os.path.join(extract_to, file) for file in files if file.endswith('.txt')]\n",
        "\n",
        "  \"\"\"\n",
        "  To extract the files, I zipped all .txt files into a single folder for better management in my program.\n",
        "  comments1k.zip might not work if trying to use this function because it have 2 folders after extracting.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "YCm0sMQMZzQc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the rule based nlp\n",
        "def process_reviews(files):\n",
        "    total_sentences = 0\n",
        "    total_tokens = 0\n",
        "    total_words_no_stop_punct = 0\n",
        "    total_comments = 0\n",
        "\n",
        "    ps = PorterStemmer() # start stemming\n",
        "    lemmatizer = WordNetLemmatizer()  # start lemmatization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    stemmed_words_l = []\n",
        "    lemmatized_words_l = []\n",
        "    original_words_l = []\n",
        "\n",
        "    for file_path in files:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            comments = f.read().strip().split(\"\\n\")  # reads entire file and splits by newlines\n",
        "\n",
        "        comments = [comment.strip() for comment in comments if comment.strip()]  # remove empty lines\n",
        "\n",
        "        total_comments += len(comments)  # count total comments\n",
        "\n",
        "        for comment in comments:\n",
        "            sentences = sent_tokenize(comment)  # sentence splitting\n",
        "            tokens = word_tokenize(comment)  # tokenization\n",
        "            words_filtered = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]  # remove the stop words condition\n",
        "\n",
        "            total_sentences += len(sentences)\n",
        "            total_tokens += len(tokens)\n",
        "            total_words_no_stop_punct += len(words_filtered)\n",
        "\n",
        "            # process for stemming and lemmatization\n",
        "            stemmed_words = [ps.stem(word) for word in words_filtered]\n",
        "            lemmatized_words = [lemmatizer.lemmatize(word) for word in words_filtered]\n",
        "\n",
        "    # store first words for observation and comparison\n",
        "    original_words_l.extend(words_filtered[:8])\n",
        "    stemmed_words_l.extend(stemmed_words[:8])\n",
        "    lemmatized_words_l.extend(lemmatized_words[:8])\n",
        "\n",
        "    # compute overall averages\n",
        "    avg_sentences = total_sentences / total_comments if total_comments > 0 else 0\n",
        "    avg_tokens = total_tokens / total_comments if total_comments > 0 else 0\n",
        "    avg_words_no_stop_punct = total_words_no_stop_punct / total_comments if total_comments > 0 else 0\n",
        "\n",
        "\n",
        "    # print results\n",
        "    print(f\"\\nTotal reviews: {total_comments:.2f}\")\n",
        "    print(f\"Avg. sentences per comment: {avg_sentences:.2f}\")\n",
        "    print(f\"Avg. tokens per comment: {avg_tokens:.2f}\")\n",
        "    print(f\"Avg. words (no stop word/punctuation): {avg_words_no_stop_punct:.2f}\")\n",
        "\n",
        "    print(\"\\nDifferences between Lemmatization and Stemming: \")\n",
        "    print(f\"Original Words: {original_words_l}\")\n",
        "    print(f\"Stemmed Words: {stemmed_words_l}\")\n",
        "    print(f\"Lemmatized Words: {lemmatized_words_l}\")\n",
        "\n",
        "    return comments\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vUEyYBKFalXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = extract_dataset('/content/reviews.zip')\n",
        "print(f\"Extracted files: {data_set}\")\n",
        "\n",
        "if data_set:\n",
        "  process_reviews(data_set)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBKXThJQgsnR",
        "outputId": "4b1159fc-e381-4110-c7b7-90d59d4e5a86",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['dataset/501_10.txt', 'dataset/638_10.txt', 'dataset/534_10.txt', 'dataset/142_8.txt', 'dataset/477_10.txt', 'dataset/346_10.txt', 'dataset/429_10.txt', 'dataset/926_7.txt', 'dataset/970_10.txt', 'dataset/622_10.txt', 'dataset/101_8.txt', 'dataset/897_10.txt', 'dataset/59_7.txt', 'dataset/676_8.txt', 'dataset/507_10.txt', 'dataset/629_9.txt', 'dataset/928_10.txt', 'dataset/143_7.txt', 'dataset/997_7.txt', 'dataset/6_10.txt', 'dataset/961_9.txt', 'dataset/574_7.txt', 'dataset/90_7.txt', 'dataset/923_9.txt', 'dataset/687_9.txt', 'dataset/313_10.txt', 'dataset/602_10.txt', 'dataset/387_8.txt', 'dataset/338_10.txt', 'dataset/770_10.txt', 'dataset/606_10.txt', 'dataset/92_9.txt', 'dataset/107_10.txt', 'dataset/761_10.txt', 'dataset/731_9.txt', 'dataset/662_8.txt', 'dataset/62_10.txt', 'dataset/120_8.txt', 'dataset/762_9.txt', 'dataset/331_10.txt', 'dataset/439_9.txt', 'dataset/122_9.txt', 'dataset/908_8.txt', 'dataset/390_10.txt', 'dataset/157_9.txt', 'dataset/236_9.txt', 'dataset/336_10.txt', 'dataset/7_7.txt', 'dataset/350_9.txt', 'dataset/871_9.txt', 'dataset/852_7.txt', 'dataset/683_10.txt', 'dataset/608_8.txt', 'dataset/461_8.txt', 'dataset/72_7.txt', 'dataset/33_7.txt', 'dataset/376_10.txt', 'dataset/258_7.txt', 'dataset/285_10.txt', 'dataset/640_10.txt', 'dataset/652_10.txt', 'dataset/389_10.txt', 'dataset/815_7.txt', 'dataset/724_10.txt', 'dataset/485_8.txt', 'dataset/941_10.txt', 'dataset/874_9.txt', 'dataset/34_8.txt', 'dataset/880_10.txt', 'dataset/435_8.txt', 'dataset/664_7.txt', 'dataset/649_10.txt', 'dataset/398_10.txt', 'dataset/408_10.txt', 'dataset/179_8.txt', 'dataset/353_9.txt', 'dataset/125_7.txt', 'dataset/30_7.txt', 'dataset/634_8.txt', 'dataset/502_10.txt', 'dataset/590_10.txt', 'dataset/635_9.txt', 'dataset/391_8.txt', 'dataset/836_8.txt', 'dataset/621_10.txt', 'dataset/476_7.txt', 'dataset/965_10.txt', 'dataset/167_7.txt', 'dataset/821_10.txt', 'dataset/977_8.txt', 'dataset/310_7.txt', 'dataset/728_10.txt', 'dataset/54_10.txt', 'dataset/364_10.txt', 'dataset/240_10.txt', 'dataset/841_10.txt', 'dataset/198_8.txt', 'dataset/428_7.txt', 'dataset/508_9.txt', 'dataset/145_10.txt', 'dataset/245_9.txt', 'dataset/809_10.txt', 'dataset/19_10.txt', 'dataset/963_8.txt', 'dataset/267_7.txt', 'dataset/656_10.txt', 'dataset/554_7.txt', 'dataset/779_10.txt', 'dataset/696_10.txt', 'dataset/289_10.txt', 'dataset/542_9.txt', 'dataset/891_10.txt', 'dataset/929_10.txt', 'dataset/570_10.txt', 'dataset/214_7.txt', 'dataset/751_9.txt', 'dataset/406_8.txt', 'dataset/614_10.txt', 'dataset/49_10.txt', 'dataset/840_9.txt', 'dataset/888_8.txt', 'dataset/91_8.txt', 'dataset/869_7.txt', 'dataset/221_9.txt', 'dataset/702_10.txt', 'dataset/81_10.txt', 'dataset/293_7.txt', 'dataset/862_7.txt', 'dataset/636_10.txt', 'dataset/952_10.txt', 'dataset/827_7.txt', 'dataset/964_8.txt', 'dataset/807_10.txt', 'dataset/308_8.txt', 'dataset/651_10.txt', 'dataset/262_8.txt', 'dataset/228_7.txt', 'dataset/163_10.txt', 'dataset/680_8.txt', 'dataset/232_10.txt', 'dataset/619_9.txt', 'dataset/75_8.txt', 'dataset/297_10.txt', 'dataset/568_7.txt', 'dataset/517_10.txt', 'dataset/248_10.txt', 'dataset/698_7.txt', 'dataset/938_9.txt', 'dataset/716_10.txt', 'dataset/113_10.txt', 'dataset/392_9.txt', 'dataset/281_10.txt', 'dataset/969_7.txt', 'dataset/991_7.txt', 'dataset/117_10.txt', 'dataset/138_7.txt', 'dataset/663_8.txt', 'dataset/681_9.txt', 'dataset/539_10.txt', 'dataset/617_7.txt', 'dataset/768_9.txt', 'dataset/847_7.txt', 'dataset/225_9.txt', 'dataset/409_10.txt', 'dataset/934_9.txt', 'dataset/902_9.txt', 'dataset/794_8.txt', 'dataset/535_10.txt', 'dataset/544_8.txt', 'dataset/369_10.txt', 'dataset/741_7.txt', 'dataset/51_10.txt', 'dataset/252_9.txt', 'dataset/753_8.txt', 'dataset/61_10.txt', 'dataset/5_10.txt', 'dataset/526_10.txt', 'dataset/650_9.txt', 'dataset/261_8.txt', 'dataset/255_10.txt', 'dataset/520_8.txt', 'dataset/440_10.txt', 'dataset/454_8.txt', 'dataset/610_9.txt', 'dataset/64_7.txt', 'dataset/601_7.txt', 'dataset/1_7.txt', 'dataset/769_8.txt', 'dataset/499_8.txt', 'dataset/595_9.txt', 'dataset/327_8.txt', 'dataset/251_10.txt', 'dataset/666_10.txt', 'dataset/541_7.txt', 'dataset/209_8.txt', 'dataset/146_10.txt', 'dataset/671_7.txt', 'dataset/962_8.txt', 'dataset/727_9.txt', 'dataset/367_10.txt', 'dataset/133_10.txt', 'dataset/613_10.txt', 'dataset/482_8.txt', 'dataset/393_8.txt', 'dataset/839_7.txt', 'dataset/438_9.txt', 'dataset/697_8.txt', 'dataset/748_9.txt', 'dataset/66_8.txt', 'dataset/433_10.txt', 'dataset/268_8.txt', 'dataset/668_7.txt', 'dataset/618_10.txt', 'dataset/631_10.txt', 'dataset/557_9.txt', 'dataset/458_10.txt', 'dataset/545_10.txt', 'dataset/648_7.txt', 'dataset/701_7.txt', 'dataset/503_10.txt', 'dataset/204_10.txt', 'dataset/446_10.txt', 'dataset/411_10.txt', 'dataset/849_7.txt', 'dataset/882_8.txt', 'dataset/689_10.txt', 'dataset/38_10.txt', 'dataset/215_8.txt', 'dataset/274_7.txt', 'dataset/705_10.txt', 'dataset/832_8.txt', 'dataset/898_10.txt', 'dataset/260_7.txt', 'dataset/275_10.txt', 'dataset/855_9.txt', 'dataset/797_8.txt', 'dataset/24_8.txt', 'dataset/639_10.txt', 'dataset/730_7.txt', 'dataset/475_10.txt', 'dataset/588_9.txt', 'dataset/277_8.txt', 'dataset/915_10.txt', 'dataset/284_10.txt', 'dataset/706_10.txt', 'dataset/166_7.txt', 'dataset/940_10.txt', 'dataset/233_7.txt', 'dataset/266_7.txt', 'dataset/765_9.txt', 'dataset/829_7.txt', 'dataset/856_7.txt', 'dataset/904_10.txt', 'dataset/518_10.txt', 'dataset/692_8.txt', 'dataset/199_10.txt', 'dataset/558_10.txt', 'dataset/430_7.txt', 'dataset/315_10.txt', 'dataset/511_10.txt', 'dataset/879_8.txt', 'dataset/418_9.txt', 'dataset/626_9.txt', 'dataset/598_9.txt', 'dataset/775_7.txt', 'dataset/986_10.txt', 'dataset/55_9.txt', 'dataset/844_8.txt', 'dataset/556_9.txt', 'dataset/586_10.txt', 'dataset/249_10.txt', 'dataset/584_8.txt', 'dataset/919_8.txt', 'dataset/665_9.txt', 'dataset/186_8.txt', 'dataset/543_10.txt', 'dataset/945_10.txt', 'dataset/854_9.txt', 'dataset/434_8.txt', 'dataset/347_10.txt', 'dataset/244_10.txt', 'dataset/183_8.txt', 'dataset/36_10.txt', 'dataset/144_8.txt', 'dataset/148_9.txt', 'dataset/559_8.txt', 'dataset/158_10.txt', 'dataset/31_8.txt', 'dataset/299_10.txt', 'dataset/538_10.txt', 'dataset/79_10.txt', 'dataset/899_7.txt', 'dataset/643_10.txt', 'dataset/755_10.txt', 'dataset/561_10.txt', 'dataset/383_10.txt', 'dataset/328_10.txt', 'dataset/250_7.txt', 'dataset/661_9.txt', 'dataset/546_10.txt', 'dataset/127_7.txt', 'dataset/2_9.txt', 'dataset/12_9.txt', 'dataset/690_9.txt', 'dataset/846_7.txt', 'dataset/304_10.txt', 'dataset/670_7.txt', 'dataset/20_9.txt', 'dataset/714_10.txt', 'dataset/58_9.txt', 'dataset/733_9.txt', 'dataset/873_8.txt', 'dataset/609_9.txt', 'dataset/551_8.txt', 'dataset/935_9.txt', 'dataset/194_8.txt', 'dataset/22_8.txt', 'dataset/395_10.txt', 'dataset/46_9.txt', 'dataset/290_9.txt', 'dataset/563_10.txt', 'dataset/40_8.txt', 'dataset/278_9.txt', 'dataset/200_10.txt', 'dataset/722_7.txt', 'dataset/424_8.txt', 'dataset/170_10.txt', 'dataset/343_10.txt', 'dataset/480_10.txt', 'dataset/173_7.txt', 'dataset/98_10.txt', 'dataset/933_10.txt', 'dataset/863_10.txt', 'dataset/67_10.txt', 'dataset/927_10.txt', 'dataset/569_10.txt', 'dataset/351_10.txt', 'dataset/612_10.txt', 'dataset/373_10.txt', 'dataset/943_10.txt', 'dataset/358_10.txt', 'dataset/718_10.txt', 'dataset/944_10.txt', 'dataset/178_7.txt', 'dataset/147_9.txt', 'dataset/28_10.txt', 'dataset/48_7.txt', 'dataset/667_8.txt', 'dataset/633_8.txt', 'dataset/746_10.txt', 'dataset/108_10.txt', 'dataset/711_10.txt', 'dataset/513_10.txt', 'dataset/719_10.txt', 'dataset/223_9.txt', 'dataset/931_10.txt', 'dataset/180_9.txt', 'dataset/151_10.txt', 'dataset/394_8.txt', 'dataset/654_10.txt', 'dataset/587_10.txt', 'dataset/975_9.txt', 'dataset/332_10.txt', 'dataset/767_9.txt', 'dataset/909_8.txt', 'dataset/920_10.txt', 'dataset/177_9.txt', 'dataset/497_10.txt', 'dataset/421_9.txt', 'dataset/686_9.txt', 'dataset/825_10.txt', 'dataset/361_10.txt', 'dataset/772_10.txt', 'dataset/537_10.txt', 'dataset/301_10.txt', 'dataset/114_10.txt', 'dataset/562_8.txt', 'dataset/831_9.txt', 'dataset/942_10.txt', 'dataset/758_9.txt', 'dataset/826_9.txt', 'dataset/567_10.txt', 'dataset/193_7.txt', 'dataset/141_9.txt', 'dataset/600_10.txt', 'dataset/276_10.txt', 'dataset/132_9.txt', 'dataset/684_10.txt', 'dataset/973_9.txt', 'dataset/527_9.txt', 'dataset/317_10.txt', 'dataset/533_10.txt', 'dataset/593_9.txt', 'dataset/11_9.txt', 'dataset/816_10.txt', 'dataset/247_10.txt', 'dataset/89_7.txt', 'dataset/213_9.txt', 'dataset/8_7.txt', 'dataset/577_8.txt', 'dataset/377_7.txt', 'dataset/771_7.txt', 'dataset/766_10.txt', 'dataset/451_10.txt', 'dataset/136_10.txt', 'dataset/878_7.txt', 'dataset/725_10.txt', 'dataset/298_8.txt', 'dataset/786_7.txt', 'dataset/42_10.txt', 'dataset/930_7.txt', 'dataset/981_7.txt', 'dataset/743_7.txt', 'dataset/334_10.txt', 'dataset/189_9.txt', 'dataset/272_10.txt', 'dataset/86_10.txt', 'dataset/205_8.txt', 'dataset/916_10.txt', 'dataset/645_10.txt', 'dataset/483_8.txt', 'dataset/191_9.txt', 'dataset/726_7.txt', 'dataset/523_10.txt', 'dataset/403_8.txt', 'dataset/630_10.txt', 'dataset/564_8.txt', 'dataset/780_7.txt', 'dataset/729_10.txt', 'dataset/74_8.txt', 'dataset/335_10.txt', 'dataset/234_10.txt', 'dataset/155_10.txt', 'dataset/540_8.txt', 'dataset/273_9.txt', 'dataset/443_10.txt', 'dataset/496_10.txt', 'dataset/522_8.txt', 'dataset/937_10.txt', 'dataset/782_9.txt', 'dataset/704_10.txt', 'dataset/359_8.txt', 'dataset/720_7.txt', 'dataset/153_10.txt', 'dataset/82_8.txt', 'dataset/890_9.txt', 'dataset/396_8.txt', 'dataset/867_8.txt', 'dataset/998_7.txt', 'dataset/987_8.txt', 'dataset/457_10.txt', 'dataset/774_8.txt', 'dataset/312_10.txt', 'dataset/717_7.txt', 'dataset/616_7.txt', 'dataset/647_10.txt', 'dataset/801_8.txt', 'dataset/235_10.txt', 'dataset/792_8.txt', 'dataset/708_8.txt', 'dataset/578_10.txt', 'dataset/906_10.txt', 'dataset/478_7.txt', 'dataset/374_10.txt', 'dataset/877_8.txt', 'dataset/17_9.txt', 'dataset/164_10.txt', 'dataset/824_8.txt', 'dataset/833_8.txt', 'dataset/357_10.txt', 'dataset/196_9.txt', 'dataset/217_8.txt', 'dataset/982_8.txt', 'dataset/460_9.txt', 'dataset/123_10.txt', 'dataset/60_8.txt', 'dataset/625_10.txt', 'dataset/830_10.txt', 'dataset/294_10.txt', 'dataset/102_10.txt', 'dataset/270_10.txt', 'dataset/325_9.txt', 'dataset/615_10.txt', 'dataset/71_10.txt', 'dataset/119_10.txt', 'dataset/382_10.txt', 'dataset/978_9.txt', 'dataset/599_10.txt', 'dataset/810_10.txt', 'dataset/459_10.txt', 'dataset/83_10.txt', 'dataset/955_7.txt', 'dataset/450_10.txt', 'dataset/444_10.txt', 'dataset/269_8.txt', 'dataset/415_7.txt', 'dataset/135_7.txt', 'dataset/655_10.txt', 'dataset/344_8.txt', 'dataset/243_10.txt', 'dataset/956_9.txt', 'dataset/422_7.txt', 'dataset/405_10.txt', 'dataset/603_10.txt', 'dataset/900_10.txt', 'dataset/581_10.txt', 'dataset/946_8.txt', 'dataset/611_10.txt', 'dataset/379_10.txt', 'dataset/239_7.txt', 'dataset/192_9.txt', 'dataset/802_10.txt', 'dataset/43_10.txt', 'dataset/65_10.txt', 'dataset/512_10.txt', 'dataset/44_8.txt', 'dataset/889_10.txt', 'dataset/995_9.txt', 'dataset/487_8.txt', 'dataset/26_9.txt', 'dataset/925_10.txt', 'dataset/959_7.txt', 'dataset/653_10.txt', 'dataset/231_10.txt', 'dataset/23_7.txt', 'dataset/218_9.txt', 'dataset/627_8.txt', 'dataset/380_10.txt', 'dataset/423_10.txt', 'dataset/300_9.txt', 'dataset/85_10.txt', 'dataset/589_10.txt', 'dataset/795_8.txt', 'dataset/69_10.txt', 'dataset/875_10.txt', 'dataset/400_10.txt', 'dataset/842_9.txt', 'dataset/131_10.txt', 'dataset/787_9.txt', 'dataset/822_9.txt', 'dataset/381_10.txt', 'dataset/521_10.txt', 'dataset/226_10.txt', 'dataset/324_8.txt', 'dataset/994_7.txt', 'dataset/330_10.txt', 'dataset/678_8.txt', 'dataset/712_9.txt', 'dataset/18_7.txt', 'dataset/103_7.txt', 'dataset/397_9.txt', 'dataset/99_8.txt', 'dataset/950_9.txt', 'dataset/745_10.txt', 'dataset/677_8.txt', 'dataset/129_9.txt', 'dataset/348_7.txt', 'dataset/340_10.txt', 'dataset/0_9.txt', 'dataset/47_8.txt', 'dataset/295_10.txt', 'dataset/176_7.txt', 'dataset/566_8.txt', 'dataset/126_10.txt', 'dataset/329_10.txt', 'dataset/314_10.txt', 'dataset/104_10.txt', 'dataset/448_10.txt', 'dataset/985_7.txt', 'dataset/45_10.txt', 'dataset/84_10.txt', 'dataset/368_10.txt', 'dataset/713_10.txt', 'dataset/812_10.txt', 'dataset/528_9.txt', 'dataset/818_10.txt', 'dataset/800_9.txt', 'dataset/912_8.txt', 'dataset/355_9.txt', 'dataset/230_9.txt', 'dataset/195_8.txt', 'dataset/319_9.txt', 'dataset/140_8.txt', 'dataset/291_10.txt', 'dataset/999_10.txt', 'dataset/4_8.txt', 'dataset/628_9.txt', 'dataset/742_9.txt', 'dataset/853_9.txt', 'dataset/798_10.txt', 'dataset/156_8.txt', 'dataset/13_7.txt', 'dataset/550_10.txt', 'dataset/817_10.txt', 'dataset/757_8.txt', 'dataset/594_9.txt', 'dataset/498_10.txt', 'dataset/464_10.txt', 'dataset/954_10.txt', 'dataset/514_10.txt', 'dataset/378_8.txt', 'dataset/744_7.txt', 'dataset/371_9.txt', 'dataset/37_9.txt', 'dataset/384_8.txt', 'dataset/150_8.txt', 'dataset/911_10.txt', 'dataset/805_9.txt', 'dataset/921_7.txt', 'dataset/263_9.txt', 'dataset/953_10.txt', 'dataset/685_8.txt', 'dataset/885_8.txt', 'dataset/88_9.txt', 'dataset/333_10.txt', 'dataset/896_10.txt', 'dataset/53_10.txt', 'dataset/94_10.txt', 'dataset/576_10.txt', 'dataset/206_10.txt', 'dataset/531_10.txt', 'dataset/404_9.txt', 'dataset/688_9.txt', 'dataset/210_10.txt', 'dataset/951_10.txt', 'dataset/905_10.txt', 'dataset/35_8.txt', 'dataset/93_10.txt', 'dataset/229_10.txt', 'dataset/375_9.txt', 'dataset/352_10.txt', 'dataset/515_10.txt', 'dataset/259_8.txt', 'dataset/524_10.txt', 'dataset/738_8.txt', 'dataset/287_9.txt', 'dataset/426_7.txt', 'dataset/447_10.txt', 'dataset/201_10.txt', 'dataset/695_10.txt', 'dataset/804_10.txt', 'dataset/445_10.txt', 'dataset/288_10.txt', 'dataset/674_10.txt', 'dataset/579_10.txt', 'dataset/431_8.txt', 'dataset/777_7.txt', 'dataset/32_10.txt', 'dataset/620_10.txt', 'dataset/932_10.txt', 'dataset/894_9.txt', 'dataset/366_9.txt', 'dataset/791_9.txt', 'dataset/491_7.txt', 'dataset/868_8.txt', 'dataset/339_10.txt', 'dataset/759_10.txt', 'dataset/345_7.txt', 'dataset/246_7.txt', 'dataset/386_7.txt', 'dataset/203_7.txt', 'dataset/843_10.txt', 'dataset/858_8.txt', 'dataset/360_10.txt', 'dataset/989_9.txt', 'dataset/365_10.txt', 'dataset/996_9.txt', 'dataset/957_10.txt', 'dataset/948_10.txt', 'dataset/254_8.txt', 'dataset/659_10.txt', 'dataset/960_7.txt', 'dataset/917_10.txt', 'dataset/134_10.txt', 'dataset/756_10.txt', 'dataset/224_10.txt', 'dataset/778_10.txt', 'dataset/596_7.txt', 'dataset/323_10.txt', 'dataset/282_9.txt', 'dataset/813_10.txt', 'dataset/412_8.txt', 'dataset/3_10.txt', 'dataset/591_10.txt', 'dataset/162_8.txt', 'dataset/292_10.txt', 'dataset/773_7.txt', 'dataset/721_10.txt', 'dataset/95_10.txt', 'dataset/63_10.txt', 'dataset/455_10.txt', 'dataset/657_10.txt', 'dataset/470_10.txt', 'dataset/111_10.txt', 'dataset/442_9.txt', 'dataset/70_9.txt', 'dataset/565_10.txt', 'dataset/401_10.txt', 'dataset/27_10.txt', 'dataset/857_8.txt', 'dataset/691_9.txt', 'dataset/417_7.txt', 'dataset/436_10.txt', 'dataset/607_10.txt', 'dataset/370_10.txt', 'dataset/222_10.txt', 'dataset/410_8.txt', 'dataset/399_9.txt', 'dataset/472_10.txt', 'dataset/637_10.txt', 'dataset/100_7.txt', 'dataset/811_10.txt', 'dataset/492_7.txt', 'dataset/349_10.txt', 'dataset/903_8.txt', 'dataset/279_9.txt', 'dataset/303_10.txt', 'dataset/699_8.txt', 'dataset/984_7.txt', 'dataset/967_7.txt', 'dataset/161_8.txt', 'dataset/504_8.txt', 'dataset/165_7.txt', 'dataset/529_10.txt', 'dataset/227_10.txt', 'dataset/739_7.txt', 'dataset/479_10.txt', 'dataset/253_7.txt', 'dataset/732_7.txt', 'dataset/641_8.txt', 'dataset/845_7.txt', 'dataset/264_7.txt', 'dataset/682_10.txt', 'dataset/242_8.txt', 'dataset/971_8.txt', 'dataset/500_9.txt', 'dataset/828_10.txt', 'dataset/553_7.txt', 'dataset/580_9.txt', 'dataset/309_9.txt', 'dataset/835_8.txt', 'dataset/484_8.txt', 'dataset/465_10.txt', 'dataset/416_8.txt', 'dataset/547_10.txt', 'dataset/316_10.txt', 'dataset/110_10.txt', 'dataset/130_9.txt', 'dataset/171_8.txt', 'dataset/968_10.txt', 'dataset/675_9.txt', 'dataset/124_10.txt', 'dataset/106_10.txt', 'dataset/80_9.txt', 'dataset/658_10.txt', 'dataset/473_9.txt', 'dataset/185_9.txt', 'dataset/860_8.txt', 'dataset/16_7.txt', 'dataset/703_10.txt', 'dataset/988_8.txt', 'dataset/152_9.txt', 'dataset/861_7.txt', 'dataset/752_7.txt', 'dataset/747_10.txt', 'dataset/257_7.txt', 'dataset/814_10.txt', 'dataset/872_9.txt', 'dataset/481_10.txt', 'dataset/644_9.txt', 'dataset/993_8.txt', 'dataset/342_10.txt', 'dataset/764_10.txt', 'dataset/21_7.txt', 'dataset/947_10.txt', 'dataset/112_10.txt', 'dataset/865_9.txt', 'dataset/471_7.txt', 'dataset/509_10.txt', 'dataset/280_8.txt', 'dataset/385_10.txt', 'dataset/388_8.txt', 'dataset/372_10.txt', 'dataset/530_10.txt', 'dataset/914_8.txt', 'dataset/819_10.txt', 'dataset/848_8.txt', 'dataset/850_8.txt', 'dataset/174_7.txt', 'dataset/489_7.txt', 'dataset/486_9.txt', 'dataset/321_10.txt', 'dataset/893_8.txt', 'dataset/763_10.txt', 'dataset/160_9.txt', 'dataset/311_9.txt', 'dataset/168_9.txt', 'dataset/548_7.txt', 'dataset/211_9.txt', 'dataset/785_9.txt', 'dataset/799_8.txt', 'dataset/859_9.txt', 'dataset/139_10.txt', 'dataset/783_10.txt', 'dataset/188_7.txt', 'dataset/723_8.txt', 'dataset/474_7.txt', 'dataset/468_7.txt', 'dataset/29_10.txt', 'dataset/322_10.txt', 'dataset/452_10.txt', 'dataset/694_9.txt', 'dataset/197_9.txt', 'dataset/585_10.txt', 'dataset/306_10.txt', 'dataset/980_7.txt', 'dataset/488_9.txt', 'dataset/363_10.txt', 'dataset/979_8.txt', 'dataset/420_7.txt', 'dataset/220_10.txt', 'dataset/549_9.txt', 'dataset/571_10.txt', 'dataset/972_9.txt', 'dataset/754_9.txt', 'dataset/734_10.txt', 'dataset/793_9.txt', 'dataset/913_10.txt', 'dataset/760_7.txt', 'dataset/516_10.txt', 'dataset/326_10.txt', 'dataset/990_9.txt', 'dataset/97_9.txt', 'dataset/582_9.txt', 'dataset/510_8.txt', 'dataset/469_7.txt', 'dataset/237_10.txt', 'dataset/910_10.txt', 'dataset/895_10.txt', 'dataset/790_9.txt', 'dataset/137_7.txt', 'dataset/552_8.txt', 'dataset/737_8.txt', 'dataset/672_9.txt', 'dataset/169_8.txt', 'dataset/41_9.txt', 'dataset/462_8.txt', 'dataset/901_8.txt', 'dataset/441_9.txt', 'dataset/184_8.txt', 'dataset/172_10.txt', 'dataset/76_7.txt', 'dataset/823_9.txt', 'dataset/73_7.txt', 'dataset/976_8.txt', 'dataset/837_7.txt', 'dataset/337_9.txt', 'dataset/96_10.txt', 'dataset/881_8.txt', 'dataset/623_10.txt', 'dataset/56_10.txt', 'dataset/532_9.txt', 'dataset/10_9.txt', 'dataset/9_7.txt', 'dataset/693_10.txt', 'dataset/128_7.txt', 'dataset/241_8.txt', 'dataset/121_10.txt', 'dataset/78_10.txt', 'dataset/715_10.txt', 'dataset/402_10.txt', 'dataset/679_10.txt', 'dataset/109_10.txt', 'dataset/851_7.txt', 'dataset/463_7.txt', 'dataset/159_10.txt', 'dataset/505_9.txt', 'dataset/118_8.txt', 'dataset/803_10.txt', 'dataset/949_8.txt', 'dataset/307_8.txt', 'dataset/796_8.txt', 'dataset/432_8.txt', 'dataset/419_7.txt', 'dataset/870_10.txt', 'dataset/175_7.txt', 'dataset/740_7.txt', 'dataset/700_8.txt', 'dataset/219_8.txt', 'dataset/820_10.txt', 'dataset/190_10.txt', 'dataset/864_9.txt', 'dataset/808_9.txt', 'dataset/413_10.txt', 'dataset/238_10.txt', 'dataset/958_9.txt', 'dataset/149_10.txt', 'dataset/494_9.txt', 'dataset/642_10.txt', 'dataset/407_10.txt', 'dataset/750_8.txt', 'dataset/453_10.txt', 'dataset/784_10.txt', 'dataset/892_10.txt', 'dataset/25_7.txt', 'dataset/573_9.txt', 'dataset/883_9.txt', 'dataset/296_10.txt', 'dataset/886_8.txt', 'dataset/265_7.txt', 'dataset/966_7.txt', 'dataset/202_10.txt', 'dataset/341_10.txt', 'dataset/15_7.txt', 'dataset/115_10.txt', 'dataset/466_8.txt', 'dataset/974_10.txt', 'dataset/207_8.txt', 'dataset/922_8.txt', 'dataset/660_9.txt', 'dataset/302_10.txt', 'dataset/936_8.txt', 'dataset/187_8.txt', 'dataset/356_10.txt', 'dataset/806_10.txt', 'dataset/597_7.txt', 'dataset/216_8.txt', 'dataset/749_10.txt', 'dataset/320_8.txt', 'dataset/838_9.txt', 'dataset/283_8.txt', 'dataset/736_10.txt', 'dataset/427_10.txt', 'dataset/710_9.txt', 'dataset/77_7.txt', 'dataset/781_9.txt', 'dataset/624_9.txt', 'dataset/992_7.txt', 'dataset/68_10.txt', 'dataset/305_8.txt', 'dataset/182_10.txt', 'dataset/449_10.txt', 'dataset/414_10.txt', 'dataset/271_10.txt', 'dataset/605_8.txt', 'dataset/493_10.txt', 'dataset/425_10.txt', 'dataset/318_10.txt', 'dataset/116_10.txt', 'dataset/604_8.txt', 'dataset/181_10.txt', 'dataset/907_8.txt', 'dataset/536_10.txt', 'dataset/362_10.txt', 'dataset/490_9.txt', 'dataset/256_9.txt', 'dataset/495_7.txt', 'dataset/154_8.txt', 'dataset/212_9.txt', 'dataset/669_7.txt', 'dataset/50_10.txt', 'dataset/884_8.txt', 'dataset/506_7.txt', 'dataset/632_10.txt', 'dataset/707_8.txt', 'dataset/467_7.txt', 'dataset/646_9.txt', 'dataset/776_7.txt', 'dataset/709_10.txt', 'dataset/887_10.txt', 'dataset/575_10.txt', 'dataset/789_7.txt', 'dataset/918_10.txt', 'dataset/354_9.txt', 'dataset/456_10.txt', 'dataset/87_10.txt', 'dataset/939_10.txt', 'dataset/583_8.txt', 'dataset/866_8.txt', 'dataset/592_10.txt', 'dataset/105_7.txt', 'dataset/555_8.txt', 'dataset/14_10.txt', 'dataset/788_8.txt', 'dataset/39_9.txt', 'dataset/876_7.txt', 'dataset/572_9.txt', 'dataset/525_10.txt', 'dataset/735_8.txt', 'dataset/286_10.txt', 'dataset/560_8.txt', 'dataset/983_7.txt', 'dataset/924_7.txt', 'dataset/519_10.txt', 'dataset/52_10.txt', 'dataset/57_10.txt', 'dataset/673_8.txt', 'dataset/834_8.txt', 'dataset/437_9.txt', 'dataset/208_9.txt']\n",
            "\n",
            "Total reviews: 1000.00\n",
            "Avg. sentences per comment: 10.70\n",
            "Avg. tokens per comment: 281.92\n",
            "Avg. words (no stop word/punctuation): 121.39\n",
            "\n",
            "Differences between Lemmatization and Stemming: \n",
            "Original Words: ['sunshine', 'boys', 'terrific', 'comedy', 'two', 'reluctantly', 'reunite', 'tv']\n",
            "Stemmed Words: ['sunshin', 'boy', 'terrif', 'comedi', 'two', 'reluctantli', 'reunit', 'tv']\n",
            "Lemmatized Words: ['sunshine', 'boy', 'terrific', 'comedy', 'two', 'reluctantly', 'reunite', 'tv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Machine Learning Basics"
      ],
      "metadata": {
        "id": "Y9vsVOmqU_fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_neural_network(x1, x2, t, w, eta=0.1):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    - Inputs: x1, x2\n",
        "    - Target: t\n",
        "    - Weights: w\n",
        "    - Learning rate: eta\n",
        "  \"\"\"\n",
        "\n",
        "  w1, w2, w3, w4, w5, w6 = w\n",
        "\n",
        "  # feed-forward process\n",
        "  h1 = (w1*x1) + (w2*x2)\n",
        "  h2 = (w3*x1) + (w4*x2)\n",
        "  y  = (w5*h1) + (w6*h2)\n",
        "\n",
        "  # compute error after the first epoch\n",
        "  E = 0.5 * (y - t) **2\n",
        "\n",
        "  # gradients\n",
        "  dy = y - t\n",
        "\n",
        "  # output layer\n",
        "  dw5 = dy * h1\n",
        "  dw6 = dy * h2\n",
        "\n",
        "  # hidden layers weights\n",
        "  dh1 = w5 * dy\n",
        "  dh2 = w6 * dy\n",
        "\n",
        "  dw1 = dh1 * x1\n",
        "  dw2 = dh1 * x2\n",
        "  dw3 = dh2 * x1\n",
        "  dw4 = dh2 * x2\n",
        "\n",
        "  # update weights based on gradient descent\n",
        "  w1 -= eta * dw1\n",
        "  w2 -= eta * dw2\n",
        "  w3 -= eta * dw3\n",
        "  w4 -= eta * dw4\n",
        "  w5 -= eta * dw5\n",
        "  w6 -= eta * dw6\n",
        "\n",
        "  updated_weights = [w1, w2, w3, w4, w5, w6]\n",
        "\n",
        "  # run new error\n",
        "  h1_new = (w1*x1) + (w2*x2)\n",
        "  h2_new = (w3*x1) + (w4*x2)\n",
        "  y_new  = (w5*h1_new) + (w6*h2_new)\n",
        "  E_new = 0.5 * (y_new - t) **2\n",
        "\n",
        "  return updated_weights, E, E_new"
      ],
      "metadata": {
        "id": "aodIm668VIB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = 1, 0.5\n",
        "t = 4\n",
        "weights = [0.5, 1.5, 2.3, 3, 1, 1]\n",
        "\n",
        "updated_weights, error, new_error = train_neural_network(x1, x2, t, weights)\n",
        "print(f\"Initial Error: {error}\")\n",
        "print(f\"Updated Error After Weight Update: {new_error}\")\n",
        "print(f\"\\nInitial Weights: {weights}\")\n",
        "print(f\"Updated Weights: {updated_weights}\")\n",
        "\n",
        "if new_error < error:\n",
        "  print(\"\\nThe error decreased after the weight update.\")\n",
        "else:\n",
        "  print(\"\\nThe error did not decrease after the weight update.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g13SMLQojntD",
        "outputId": "c3445093-88a0-4b52-f36e-70352c8a2c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Error: 0.5512499999999998\n",
            "Updated Error After Weight Update: 0.3388021092883306\n",
            "\n",
            "Initial Weights: [0.5, 1.5, 2.3, 3, 1, 1]\n",
            "Updated Weights: [0.395, 1.4475, 2.195, 2.9475, 0.86875, 0.601]\n",
            "\n",
            "The error decreased after the weight update.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Text Annotation\n",
        "Part 1: Entity and Sentiment Annotation"
      ],
      "metadata": {
        "id": "SujlaH4tnh4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# entity annotation\n",
        "def named_entity_annotation(text):\n",
        "  doc = nlp(text)\n",
        "  entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "  with open('ner_annotations.txt', 'w') as f:\n",
        "    for entity, label in entities:\n",
        "      f.write(f\"{entity} - {label}\\n\")\n",
        "\n",
        "  return entities"
      ],
      "metadata": {
        "id": "HgfxW-_GnvtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentiment annotation\n",
        "def sentiment_annotation(text):\n",
        "  blob = TextBlob(text)\n",
        "  sentiment_score = blob.sentiment.polarity\n",
        "  sentiment_label = \"Positive\" if sentiment_score > 0 else \"Negative\" if sentiment_score < 0 else \"Neutral\"\n",
        "\n",
        "  with open('sentiment_annotations.txt', 'w') as f:\n",
        "    f.write(f\"Sentiment: {sentiment_label} Score: {sentiment_score:.3f}\\n\")\n",
        "\n",
        "  return sentiment_label, sentiment_score"
      ],
      "metadata": {
        "id": "EL59LVS2oz3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provided texts\n",
        "ner_text = \"Barack Obama was the 44th President of the United States. He was born in Hawaii and studied law at Harvard University.\"\n",
        "sentiment_text = \"\"\"De Niro has the ability to make every role he portrays into acting gold.\n",
        "    He gives a great performance in this film and there is a great scene where he has to take his\n",
        "    father to a home for elderly people because he can't care for him anymore that will break your heart.\n",
        "    I will say you won't see much bette acting anywhere.\"\"\"\n",
        "\n",
        "ner_results = named_entity_annotation(ner_text)\n",
        "print(\"\\nNamed Entity Recognition Results:\")\n",
        "for entity, label in ner_results:\n",
        "  print(f\"{entity} - {label}\")\n",
        "\n",
        "sentiment_label, sentiment_score = sentiment_annotation(sentiment_text)\n",
        "print(\"\\nSentiment Analysis Results:\")\n",
        "print(f\"Sentiment: {sentiment_label}, Score: {sentiment_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa7CIdwepkJe",
        "outputId": "db166021-673b-4b2e-cd65-6b5ec05b232b",
        "collapsed": true
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entity Recognition Results:\n",
            "Barack Obama - PERSON\n",
            "44th - ORDINAL\n",
            "the United States - GPE\n",
            "Hawaii - GPE\n",
            "Harvard University - ORG\n",
            "\n",
            "Sentiment Analysis Results:\n",
            "Sentiment: Positive, Score: 0.360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Active Learning"
      ],
      "metadata": {
        "id": "38DnblfAsm6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "# Split the dataset into initial training set and pool set\n",
        "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n",
        "# Initialize the active learning loop\n",
        "iterations = 10\n",
        "batch_size = 10\n",
        "model = LogisticRegression(random_state=42)\n",
        "for i in range(iterations):\n",
        " print(\"Iteration {}:\".format(i+1))\n",
        "\n",
        " # Train the model on the current training set\n",
        " model.fit(X_train, y_train)\n",
        "\n",
        " # Predict the labels of the unlabeled instances in the pool set\n",
        " y_pool_pred = model.predict(X_pool)\n",
        "\n",
        " ### below\n",
        " y_pool_prob = model.predict_proba(X_pool)\n",
        " entropy = -np.sum(y_pool_prob * np.log(y_pool_prob), axis=1)\n",
        " query_idx = np.argsort(entropy)[-batch_size:]\n",
        " ### above\n",
        "\n",
        " X_query = X_pool[query_idx]\n",
        " y_query = y_pool[query_idx]\n",
        " # Add the labeled instances to the training set and remove them from the pool set\n",
        " X_train = np.concatenate([X_train, X_query])\n",
        " y_train = np.concatenate([y_train, y_query])\n",
        " X_pool = np.delete(X_pool, query_idx, axis=0)\n",
        " y_pool = np.delete(y_pool, query_idx)\n",
        " # Compute and print the accuracy of the model on the test set\n",
        " y_test_pred = model.predict(X_pool)\n",
        " accuracy = accuracy_score(y_pool, y_test_pred)\n",
        " print(\"Accuracy: {:.3f}\\n\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XEL6_M39srNH",
        "outputId": "211a77d1-16b2-4ab1-bd46-bc09219cc223"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1:\n",
            "Accuracy: 0.828\n",
            "\n",
            "Iteration 2:\n",
            "Accuracy: 0.834\n",
            "\n",
            "Iteration 3:\n",
            "Accuracy: 0.851\n",
            "\n",
            "Iteration 4:\n",
            "Accuracy: 0.864\n",
            "\n",
            "Iteration 5:\n",
            "Accuracy: 0.874\n",
            "\n",
            "Iteration 6:\n",
            "Accuracy: 0.879\n",
            "\n",
            "Iteration 7:\n",
            "Accuracy: 0.881\n",
            "\n",
            "Iteration 8:\n",
            "Accuracy: 0.883\n",
            "\n",
            "Iteration 9:\n",
            "Accuracy: 0.886\n",
            "\n",
            "Iteration 10:\n",
            "Accuracy: 0.894\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \"\"\"\n",
        " 2a)\n",
        " y_pool_prob = model.predict_proba(X_pool) # ----> This line of code gets probability scores for each instance in pool set.\n",
        " entropy = -np.sum(y_pool_prob * np.log(y_pool_prob), axis=1) # ----> Takes measurement of uncertainty of each prediction.\n",
        " query_idx = np.argsort(entropy)[-batch_size:] # ----> Selects the batch_size samples with highest uncertainty and let the model learn from them.\n",
        "\n",
        "  ** This approach focuses on samples where the model is most uncertain. With this approach it helps improve the model's performance.\n",
        "  Instead of labeling easy samples, it takes the most uncertain ones.\n",
        " \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "collapsed": true,
        "id": "rHxWirRltjFW",
        "outputId": "a1411152-5329-4609-e6af-7289253eefe7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ny_pool_prob = model.predict_proba(X_pool) # ----> This line of code gets probability scores for each instance in pool set.\\nentropy = -np.sum(y_pool_prob * np.log(y_pool_prob), axis=1) # ----> Takes measurement of uncertainty of each prediction.\\nquery_idx = np.argsort(entropy)[-batch_size:] # ----> Selects the batch_size samples with highest uncertainty and let the model learn from them.\\n\\n ** This approach focuses on samples where the model is most uncertain. With this approach it helps improve the model's performance.\\n Instead of labeling easy samples, it takes the most uncertain ones.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Alternative Strategy: Least Confidence Sampling\n",
        "\n",
        "y_pool_prob = model.predict_proba(X_pool)\n",
        "max_confidence = np.max(y_pool_prob, axis=1)  # get max probability for each instance\n",
        "query_idx = np.argsort(max_confidence)[:batch_size]  # select lowest confidence samples\n",
        "\"\"\"\n",
        "2a) ** The original approach (Entropy Sampling) is better when the dataset is imbalanced because it ensures the model focuses on\n",
        "different hard cases.\n",
        "The new strategy proposed (Least Confidence Sampling) is faster and works better when computational efficiency is a concern.\n",
        "\n",
        "2b) ** It depends on the batch_size, in this case would be 10. The pros of having smaller labelled reviews is that there is a\n",
        "more focused learning, it's also less work for the annotator. In terms of cons, it would require more iterations to achieve\n",
        "good performance, which will make the process a bit more slower.\n",
        "\n",
        "The pros of having a larger labelled reviews is that there is a faster convergence because the model can learn from a larger set\n",
        "of labelled data, hence fewer iterations will be needed. In the cons we might have more manual work, which involves more\n",
        "annotations per iteration.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "collapsed": true,
        "id": "S41UdYJKu2Xm",
        "outputId": "d28d5182-ac50-417b-9693-089548690489"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n** The original approach (Entropy Sampling) is better when the dataset is imbalanced because it ensures the model focuses on \\ndifferent hard cases.\\nThe new strategy proposed (Least Confidence Sampling) is faster and works better when computational efficiency is a concern.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}